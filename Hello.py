import streamlit as st
import anthropic
import os
import tiktoken


class BaseEmbeddings:
    """
    Base class for embeddings
    """
    def __init__(self, path: str, is_api: bool) -> None:
        self.path = path
        self.is_api = is_api
    
    def get_embedding(self, text: str, model: str) -> List[float]:
        raise NotImplementedError
    
    @classmethod
    def cosine_similarity(cls, vector1: List[float], vector2: List[float]) -> float:
        """
        calculate cosine similarity between two vectors
        """
        dot_product = np.dot(vector1, vector2)
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
        if not magnitude:
            return 0
        return dot_product / magnitude

class OpenAIEmbedding(BaseEmbeddings):
    """
    class for OpenAI embeddings
    """
    def __init__(self, path: str = '', is_api: bool = True) -> None:
        super().__init__(path, is_api)
        if self.is_api:
            from openai import OpenAI
            self.client = OpenAI()
            self.client.api_key = os.getenv("OPENAI_API_KEY")
            self.client.base_url = os.getenv("OPENAI_BASE_URL")
    
    def get_embedding(self, text: str, model: str = "text-embedding-3-large") -> List[float]:
        if self.is_api:
            text = text.replace("\n", " ")
            return self.client.embeddings.create(input=[text], model=model).data[0].embedding
        else:
            raise NotImplementedError


class VectorStore:
    def __init__(self, document: List[str] = ['']) -> None:
        self.document = document

    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:
        
        self.vectors = []
        for doc in tqdm(self.document, desc="Calculating embeddings"):
            self.vectors.append(EmbeddingModel.get_embedding(doc))
        return self.vectors

    def persist(self, path: str = 'storage'):
        if not os.path.exists(path):
            os.makedirs(path)
        with open(f"{path}/doecment.json", 'w', encoding='utf-8') as f:
            json.dump(self.document, f, ensure_ascii=False)
        if self.vectors:
            with open(f"{path}/vectors.json", 'w', encoding='utf-8') as f:
                json.dump(self.vectors, f)

    def load_vector(self, path: str = 'storage'):
        with open(f"{path}/vectors.json", 'r', encoding='utf-8') as f:
            self.vectors = json.load(f)
        with open(f"{path}/doecment.json", 'r', encoding='utf-8') as f:
            self.document = json.load(f)

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        return BaseEmbeddings.cosine_similarity(vector1, vector2)

    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:
        query_vector = EmbeddingModel.get_embedding(query)
        result = np.array([self.get_similarity(query_vector, vector)
                          for vector in self.vectors])
        return np.array(self.document)[result.argsort()[-k:][::-1]].tolist()



enc = tiktoken.get_encoding("cl100k_base")
class ReadFiles:
    """
    class to read files
    """

    def __init__(self, path: str) -> None:
        self._path = path
        self.file_list = self.get_files()

    def get_files(self):
        # argsï¼šdir_pathï¼Œç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
        file_list = []
        for filepath, dirnames, filenames in os.walk(self._path):
            # os.walk å‡½æ•°å°†é€’å½’éå†æŒ‡å®šæ–‡ä»¶å¤¹
            for filename in filenames:
                # é€šè¿‡åç¼€ååˆ¤æ–­æ–‡ä»¶ç±»å‹æ˜¯å¦æ»¡è¶³è¦æ±‚               
                if filename.endswith(".txt"):
                    file_list.append(os.path.join(filepath, filename))
                
        return file_list

    def get_content(self, max_token_len: int = 600, cover_content: int = 150):
        docs = []
        # è¯»å–æ–‡ä»¶å†…å®¹
        for file in self.file_list:
            content = self.read_file_content(file)
            chunk_content = self.get_chunk(
                content, max_token_len=max_token_len, cover_content=cover_content)
            docs.extend(chunk_content)
        return docs

    @classmethod
    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150):
        chunk_text = []

        curr_len = 0
        curr_chunk = ''

        token_len = max_token_len - cover_content
        lines = text.splitlines()  # å‡è®¾ä»¥æ¢è¡Œç¬¦åˆ†å‰²æ–‡æœ¬ä¸ºè¡Œ

        for line in lines:
            line = line.replace(' ', '')
            line_len = len(enc.encode(line))
            if line_len > max_token_len:
                # å¦‚æœå•è¡Œé•¿åº¦å°±è¶…è¿‡é™åˆ¶ï¼Œåˆ™å°†å…¶åˆ†å‰²æˆå¤šä¸ªå—
                num_chunks = (line_len + token_len - 1) // token_len
                for i in range(num_chunks):
                    start = i * token_len
                    end = start + token_len
                    # é¿å…è·¨å•è¯åˆ†å‰²
                    while not line[start:end].rstrip().isspace():
                        start += 1
                        end += 1
                        if start >= line_len:
                            break
                    curr_chunk = curr_chunk[-cover_content:] + line[start:end]
                    chunk_text.append(curr_chunk)
                # å¤„ç†æœ€åä¸€ä¸ªå—
                start = (num_chunks - 1) * token_len
                curr_chunk = curr_chunk[-cover_content:] + line[start:end]
                chunk_text.append(curr_chunk)
                
            if curr_len + line_len <= token_len:
                curr_chunk += line
                curr_chunk += '\n'
                curr_len += line_len
                curr_len += 1
            else:
                chunk_text.append(curr_chunk)
                curr_chunk = curr_chunk[-cover_content:]+line
                curr_len = line_len + cover_content

        if curr_chunk:
            chunk_text.append(curr_chunk)

        return chunk_text

    @classmethod
    def read_file_content(cls, file_path: str):
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
        if file_path.endswith('.txt'):
            return cls.read_text(file_path)
        else:
            raise ValueError("Unsupported file type")

    @classmethod
    def read_text(cls, file_path: str):
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()

PROMPT_TEMPLATE = dict(
    RAG_PROMPT_TEMPALTE="""ä½¿ç”¨ä»¥ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
        é—®é¢˜: {question}
        å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
        Â·Â·Â·
        {context}
        Â·Â·Â·
        å¦‚æœç»™å®šçš„ä¸Šä¸‹æ–‡æ— æ³•è®©ä½ åšå‡ºå›ç­”ï¼Œè¯·å›ç­”æ•°æ®åº“ä¸­æ²¡æœ‰è¿™ä¸ªå†…å®¹ï¼Œä½ ä¸çŸ¥é“ã€‚
        æœ‰ç”¨çš„å›ç­”:""",
    InternLM_PROMPT_TEMPALTE="""å…ˆå¯¹ä¸Šä¸‹æ–‡è¿›è¡Œå†…å®¹æ€»ç»“,å†ä½¿ç”¨ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
        é—®é¢˜: {question}
        å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
        Â·Â·Â·
        {context}
        Â·Â·Â·
        å¦‚æœç»™å®šçš„ä¸Šä¸‹æ–‡æ— æ³•è®©ä½ åšå‡ºå›ç­”ï¼Œè¯·å›ç­”æ•°æ®åº“ä¸­æ²¡æœ‰è¿™ä¸ªå†…å®¹ï¼Œä½ ä¸çŸ¥é“ã€‚
        æœ‰ç”¨çš„å›ç­”:"""
)

class BaseModel:
    def __init__(self, path: str = '') -> None:
        self.path = path

    def chat(self, prompt: str, history: List[dict], content: str) -> str:
        pass

    def load_model(self):
        pass

class OpenAIChat(BaseModel):
    def __init__(self, path: str = '', model: str = "gpt-3.5-turbo-1106", api_key) -> None:
        super().__init__(path)
        self.model = model
        self.api_key = api_key

    def chat(self, prompt: str, history: List[dict], content: str) -> str:
        from openai import OpenAI
        client = OpenAI()
        client.api_key = self.api_key 
        # client.base_url = os.getenv("OPENAI_BASE_URL")
        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})
        response = client.chat.completions.create(
            model=self.model,
            messages=history,
            max_tokens=150,
            temperature=0.1
        )
        return response.choices[0].message.content


with st.sidebar:
    anthropic_api_key = st.text_input("Anthropic API Key", key="file_qa_api_key", type="password")
    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/1_File_Q%26A.py)"
    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"

st.title("ğŸ“ File Q&A with Anthropic")
uploaded_file = st.file_uploader("Upload an article", type=("txt", "md"))
question = st.text_input(
    "Ask something about the article",
    placeholder="Can you give me a short summary?",
    disabled=not uploaded_file,
)

if uploaded_file and question and not anthropic_api_key:
    st.info("Please add your Anthropic API key to continue.")

if uploaded_file and question and anthropic_api_key:
    # article = uploaded_file.read().decode()
    # prompt = f"""{anthropic.HUMAN_PROMPT} Here's an article:\n\n<article>
    # {article}\n\n</article>\n\n{question}{anthropic.AI_PROMPT}"""

    # client = anthropic.Client(api_key=anthropic_api_key)
    # response = client.completions.create(
    #     prompt=prompt,
    #     stop_sequences=[anthropic.HUMAN_PROMPT],
    #     model="claude-1",  # "claude-2" for Claude 2 model
    #     max_tokens_to_sample=100,
    # )
    # st.write("### Answer")
    # st.write(response.completion)
    docs = ReadFiles(uploaded_file).get_content(max_token_len=600, cover_content=150) # è·å¾—dataç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å†…å®¹å¹¶åˆ†å‰²
    vector = VectorStore(docs)
    embedding = OpenAIEmbedding() # åˆ›å»ºEmbeddingModel
    vector.get_vector(EmbeddingModel=embedding)
    vector.persist(path='storage') # å°†å‘é‡å’Œæ–‡æ¡£å†…å®¹ä¿å­˜åˆ°storageç›®å½•ä¸‹ï¼Œä¸‹æ¬¡å†ç”¨å°±å¯ä»¥ç›´æ¥åŠ è½½æœ¬åœ°çš„æ•°æ®åº“

    # vector.load_vector('./storage') # åŠ è½½æœ¬åœ°çš„æ•°æ®åº“

    if question:
        content = vector.query(question, EmbeddingModel=embedding, k=1)[0]
        chat = OpenAIChat(model='gpt-3.5-turbo-1106')
        print(chat.chat(question, [], content))